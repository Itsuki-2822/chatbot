{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mira/VScode/chatbot/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/mira/VScode/chatbot/data/my_profile.csv',header=None,names=['text','Category'])\n",
    "\n",
    "# OpenAIEmbeddings インスタンスを作成\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 'text_column' の各セルをベクトル化してリストに保存\n",
    "df['vectorized'] = df['text'].apply(lambda x: embeddings.embed_query(x))\n",
    "\n",
    "# ベクトルリストをデータフレームに展開（各要素を別々のカラムに）\n",
    "vectorized_df = pd.DataFrame(df['vectorized'].tolist(), index=df.index)\n",
    "\n",
    "# 元のDataFrameとベクトル化したDataFrameを結合\n",
    "output_df = pd.concat([df.drop(columns=['vectorized']), vectorized_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'deletion_protection': 'disabled',\n",
       "              'dimension': 1536,\n",
       "              'host': 'sample-db-rdebc4f.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'sample-db',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_index = pc.Index(\"sample-db\")\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = output_df.iloc[:, 0]\n",
    "original_category = output_df.iloc[:, 1]\n",
    "vectorized_data_only = output_df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vectorized_data_only)):\n",
    "    pinecone_index.upsert(\n",
    "        vectors = [\n",
    "            {\n",
    "                'id': str(i+1),\n",
    "                'values': vectorized_data_only.T[i],\n",
    "                'metadata': {\"text\": original_texts[i], \"Category\": original_category[i]}\n",
    "            }\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'deletion_protection': 'disabled',\n",
       "              'dimension': 1536,\n",
       "              'host': 'sample-db-rdebc4f.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'sample-db',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=\"aws\"\n",
    "index_name = \"sample-db\"\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#vectorstore = PineconeStore.from_documents(index_name, embeddings,\"text\")\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings,pinecone_api_key=pinecone_api_key)\n",
    "#retriever = PineconeVectorStore(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回答: 趣味は読書、散歩、プログラミングです！特にプログラミングは、最初は就活のために始めたんですが、今ではすっかり楽しくなって、自分のもう一つの趣味になりました！エディタと向き合う時間が増えて、毎日新しいことを学んでいます！\n",
      "回答: 主にPython、Java、JavaScript（TypeScript）を使用しています！特に機械学習のプロジェクトではPythonが多いですね。\n",
      "回答: はい、機械学習系のインターンが多いです！特に、Zigexnでの長期インターンシップやAlmondoでのLLM開発のプロジェクトを通じて、実践的な経験を積んでいます！\n",
      "回答: Zigexnでの長期インターンシップでは、機械学習エンジニアとしてさまざまなプロジェクトに取り組んでいます！具体的には、データ分析やモデルの構築、そして実際のビジネス課題に対する機械学習の適用を行っています。特に、自分専用のChatBotの作成を通じて技術力を高めることに力を入れています！このプロジェクトは、自分の技術力を外にアピールする良い機会にもなっています！\n",
      "回答: はい！AlmondoでのLLM開発のプロジェクトでは、プロジェクトマネージャーとしてチームをリードし、効率的な開発を進めています！さまざまな技術的課題に取り組みながら、最適なソリューションを見つけることに注力しています。チームメンバーとのコミュニケーションも大切にしていて、みんなで協力しながら進めています！\n",
      "回答: やめる理由は何ですか？\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# プロンプトテンプレートの定義\n",
    "prompt_template = \"\"\"Use the following pieces of context: {context} \n",
    "感情が伝わるように「！」など感情表現豊かにしてください。全てに「！」をつけないでください、適切なタイミングで使うようにして。また一問一答のように質問にだけ応えて。: {question}\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# completion llm  \n",
    "llm = ChatOpenAI(  \n",
    "    model_name='gpt-4o-mini',  \n",
    "    temperature=0,\n",
    ")  \n",
    "\n",
    "# RetrievalQAの設定\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT, \"document_variable_name\": \"context\"} \n",
    ")  \n",
    "\n",
    "# インタラクティブな会話ループ\n",
    "context = \"\"  # 初期文脈\n",
    "while True:\n",
    "    q = input(\"あなたの質問: \")  # ユーザーからの質問を取得\n",
    "    result = qa.invoke({\"query\": q, \"context\": context})  # 質問を実行\n",
    "    print(\"回答:\", result['result'])  # モデルの応答を表示\n",
    "    \n",
    "    # 次の質問のために文脈を更新\n",
    "    context += f\"\\n質問: {q}\\n回答: {result['result']}\"\n",
    "    \n",
    "    # 終了条���\n",
    "    if q.lower() in [\"exit\", \"終了\", \"やめる\"]:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'長期インターンシップでは、機械学習エンジニアとしてさまざまなプロジェクトに取り組んでいます！具体的には、データの前処理やモデルの構築、評価などを行い、実際のビジネスに役立つ機械学習ソリューションを開発しています。日々新しい技術や知識を学びながら、チームと協力して成果を出すことにやりがいを感じています！'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q='長期インターンシップで何をしていますか'\n",
    "result = qa.invoke({\"query\": q})\n",
    "display(\"Answer:\", result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

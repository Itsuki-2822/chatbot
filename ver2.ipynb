{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQA \n",
    "from google.oauth2 import service_account\n",
    "from io import BytesIO\n",
    "from googleapiclient.discovery import build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_info({\n",
    "    \"type\": os.getenv(\"GOOGLE_TYPE\"),\n",
    "    \"project_id\": os.getenv(\"GOOGLE_PROJECT_ID\"),\n",
    "    \"private_key_id\": os.getenv(\"GOOGLE_PRIVATE_KEY_ID\"),\n",
    "    \"private_key\": os.getenv(\"GOOGLE_PRIVATE_KEY\").replace(\"\\\\n\", \"\\n\"),\n",
    "    \"client_email\": os.getenv(\"GOOGLE_CLIENT_EMAIL\"),\n",
    "    \"client_id\": os.getenv(\"GOOGLE_CLIENT_ID\"),\n",
    "    \"auth_uri\": os.getenv(\"GOOGLE_AUTH_URI\"),\n",
    "    \"token_uri\": os.getenv(\"GOOGLE_TOKEN_URI\"),\n",
    "    \"auth_provider_x509_cert_url\": os.getenv(\"GOOGLE_AUTH_PROVIDER_X509_CERT_URL\"),\n",
    "    \"client_x509_cert_url\": os.getenv(\"GOOGLE_CLIENT_X509_CERT_URL\")\n",
    "})\n",
    "\n",
    "service = build('drive', 'v3', credentials=credentials)\n",
    "file_id = os.getenv('FILE_ID')\n",
    "request = service.files().get_media(fileId=file_id)\n",
    "file_data = BytesIO(request.execute())\n",
    "df = pd.read_csv(file_data, header=None, names=['text', 'Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 'text_column' の各セルをベクトル化してリストに保存\n",
    "df['vectorized'] = df['text'].apply(lambda x: embeddings.embed_query(x))\n",
    "\n",
    "# ベクトルリストをデータフレームに展開（各要素を別々のカラムに）\n",
    "vectorized_df = pd.DataFrame(df['vectorized'].tolist(), index=df.index)\n",
    "\n",
    "# 元のDataFrameとベクトル化したDataFrameを結合\n",
    "output_df = pd.concat([df.drop(columns=['vectorized']), vectorized_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'deletion_protection': 'disabled',\n",
       "              'dimension': 1536,\n",
       "              'host': 'sample-db-rdebc4f.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'sample-db',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_index = pc.Index(\"sample-db\")\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = output_df.iloc[:, 0]\n",
    "original_category = output_df.iloc[:, 1]\n",
    "vectorized_data_only = output_df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vectorized_data_only)):\n",
    "    pinecone_index.upsert(\n",
    "        vectors = [\n",
    "            {\n",
    "                'id': str(i+1),\n",
    "                'values': vectorized_data_only.T[i],\n",
    "                'metadata': {\"text\": original_texts[i], \"Category\": original_category[i]}\n",
    "            }\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'deletion_protection': 'disabled',\n",
       "              'dimension': 1536,\n",
       "              'host': 'sample-db-rdebc4f.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'sample-db',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=\"aws\"\n",
    "index_name = \"sample-db\"\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#vectorstore = PineconeStore.from_documents(index_name, embeddings,\"text\")\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings,pinecone_api_key=pinecone_api_key)\n",
    "#retriever = PineconeVectorStore(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'休みの日は読書をしたり、散歩を楽しんだり、プログラミングに没頭したりしています！特に自然の中を歩くのが好きです！'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# プロンプトテンプレートの定義\n",
    "prompt_template = \"\"\"Use the following pieces of context: {context} \n",
    "感情が伝わるように「！」など感情表現豊かにしてください。全てに「！」をつけないでください、適切なタイミングで使うようにして。また一問一答のように質問にだけ応えて。: {question}\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# completion llm  \n",
    "llm = ChatOpenAI(  \n",
    "    model_name='gpt-4o-mini',  \n",
    "    temperature=0,\n",
    ")  \n",
    "\n",
    "# RetrievalQAの設定\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT, \"document_variable_name\": \"context\"} \n",
    ")  \n",
    "\n",
    "# クエリの実行\n",
    "q = '休みの日は何をしていますか？'\n",
    "result = qa.invoke({\"query\": q})\n",
    "display(\"Answer:\", result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.chains import RetrievalQA\\n\\n# プロンプトテンプレートの定義\\nprompt_template = \"\"\"Use the following pieces of context: {context} \\n感情が伝わるように「！」など感情表現豊かにしてください。全てに「！」をつけないでください、適切なタイミングで使うようにして。また一問一答のように質問にだけ応えて。: {question}\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template, \\n    input_variables=[\"context\", \"question\"]\\n)\\n\\n# completion llm  \\nllm = ChatOpenAI(  \\n    model_name=\\'gpt-4o-mini\\',  \\n    temperature=0,\\n)  \\n\\n# RetrievalQAの設定\\nqa = RetrievalQA.from_chain_type(  \\n    llm=llm,  \\n    chain_type=\"stuff\",  \\n    retriever=vectorstore.as_retriever(),\\n    chain_type_kwargs={\"prompt\": PROMPT, \"document_variable_name\": \"context\"} \\n)  \\n\\n# インタラクティブな会話ループ\\ncontext = \"\"  # 初期文脈\\nwhile True:\\n    q = input(\"あなたの質問: \")  # ユーザーからの質問を取得\\n    result = qa.invoke({\"query\": q, \"context\": context})  # 質問を実行\\n    print(\"回答:\", result[\\'result\\'])  # モデルの応答を表示\\n    \\n    # 次の質問のために文脈を更新\\n    context += f\"\\n質問: {q}\\n回答: {result[\\'result\\']}\"\\n    \\n    # 終了条���\\n    if q.lower() in [\"exit\", \"終了\", \"やめる\"]:\\n        break\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# プロンプトテンプレートの定義\n",
    "prompt_template = \"\"\"Use the following pieces of context: {context} \n",
    "感情が伝わるように「！」など感情表現豊かにしてください。全てに「！」をつけないでください、適切なタイミングで使うようにして。また一問一答のように質問にだけ応えて。: {question}\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# completion llm  \n",
    "llm = ChatOpenAI(  \n",
    "    model_name='gpt-4o-mini',  \n",
    "    temperature=0,\n",
    ")  \n",
    "\n",
    "# RetrievalQAの設定\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT, \"document_variable_name\": \"context\"} \n",
    ")  \n",
    "\n",
    "# インタラクティブな会話ループ\n",
    "context = \"\"  # 初期文脈\n",
    "while True:\n",
    "    q = input(\"あなたの質問: \")  # ユーザーからの質問を取得\n",
    "    result = qa.invoke({\"query\": q, \"context\": context})  # 質問を実行\n",
    "    print(\"回答:\", result['result'])  # モデルの応答を表示\n",
    "    \n",
    "    # 次の質問のために文脈を更新\n",
    "    context += f\"\\n質問: {q}\\n回答: {result['result']}\"\n",
    "    \n",
    "    # 終了条���\n",
    "    if q.lower() in [\"exit\", \"終了\", \"やめる\"]:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'いいえ、彼女はいません！今は勉強に集中しています！'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q='彼女はいますか'\n",
    "result = qa.invoke({\"query\": q})\n",
    "display(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'僕の大学は東京国際工科専門職大学です！学部は工科学部で、学科は情報工学科です！'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q='大学と学部と学科を教えてください'\n",
    "result = qa.invoke({\"query\": q})\n",
    "display(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'はい！研究室のリンクはこちらです！[塩尻（斎藤）亜希 研究室](https://shiojirilab.wixsite.com/shiojirilab/%E3%82%A2%E3%82%AF%E3%82%BB%E3%82%B9)です！ぜひ訪れてみてください！'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q='研究室のリンクとかありますか？'\n",
    "result = qa.invoke({\"query\": q})\n",
    "display(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/mira/VScode/chatbot/data/my_profile.csv',header=None,names=['text','Category'])\n",
    "\n",
    "# OpenAIEmbeddings インスタンスを作成\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 'text_column' の各セルをベクトル化してリストに保存\n",
    "df['vectorized'] = df['text'].apply(lambda x: embeddings.embed_query(x))\n",
    "\n",
    "# ベクトルリストをデータフレームに展開（各要素を別々のカラムに）\n",
    "vectorized_df = pd.DataFrame(df['vectorized'].tolist(), index=df.index)\n",
    "\n",
    "# 元のDataFrameとベクトル化したDataFrameを結合\n",
    "output_df = pd.concat([df.drop(columns=['vectorized']), vectorized_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'deletion_protection': 'disabled',\n",
       "              'dimension': 1536,\n",
       "              'host': 'sample-db-rdebc4f.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'sample-db',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_index = pc.Index(\"sample-db\")\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = output_df.iloc[:, 0]\n",
    "original_category = output_df.iloc[:, 1]\n",
    "vectorized_data_only = output_df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vectorized_data_only)):\n",
    "    pinecone_index.upsert(\n",
    "        vectors = [\n",
    "            {\n",
    "                'id': str(i+1),\n",
    "                'values': vectorized_data_only.T[i],\n",
    "                'metadata': {\"text\": original_texts[i], \"Category\": original_category[i]}\n",
    "            }\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'deletion_protection': 'disabled',\n",
       "              'dimension': 1536,\n",
       "              'host': 'sample-db-rdebc4f.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'sample-db',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=\"aws\"\n",
    "index_name = \"sample-db\"\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#vectorstore = PineconeStore.from_documents(index_name, embeddings,\"text\")\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings,pinecone_api_key=pinecone_api_key)\n",
    "#retriever = PineconeVectorStore(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'はい、塩尻（斎藤）亜希研究室に所属しています。'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# completion llm  \n",
    "llm = ChatOpenAI(  \n",
    "    model_name='gpt-4o-mini',  \n",
    "    temperature=0.4,\n",
    ")  \n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")  \n",
    "#query = \"趣味は？？\"  \n",
    "#qa.run(query)\n",
    "\n",
    "q='研究室とかに所属してますか？'\n",
    "result = qa.invoke({\"query\": q})\n",
    "display(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'長期インターンシップでは、株式会社Zigexnにて機械学習エンジニアとしての業務に従事しています。また、業務委託として株式会社AlmondoでLLM開発のプロジェクトマネージャー（PjM）をしています。'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q='長期インターンシップで何をしていますか'\n",
    "result = qa.invoke({\"query\": q})\n",
    "display(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieval_qa = RetrievalQA.from_chain_type(\\n        llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\\n        chain_type=\"refine\",\\n        retriever=vectorstore.as_retriever(),\\n        return_source_documents=True\\n)\\nq=\\'趣味はなんですか？\\'\\nresult = retrieval_qa.invoke({\"query\": q})\\ndisplay(\"Answer:\", result[\\'result\\'])'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"retrieval_qa = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        chain_type=\"refine\",\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        return_source_documents=True\n",
    ")\n",
    "q='趣味はなんですか？'\n",
    "result = retrieval_qa.invoke({\"query\": q})\n",
    "display(\"Answer:\", result['result'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# CSVファイルを読み込み、列名を指定\\ndef load_data(filepath):\\n    return pd.read_csv(filepath, header=None, names=[\\'text\\', \\'Category\\'])\\n\\n# テキストデータをベクトル化\\ndef vectorize_text(df, embeddings):\\n    df[\\'vectorized\\'] = df[\\'text\\'].apply(lambda x: embeddings.embed_query(x))\\n    return df\\n\\n# ベクトルデータをDataFrameに展開\\ndef expand_vectors(df):\\n    vectorized_df = pd.DataFrame(df[\\'vectorized\\'].tolist(), index=df.index)\\n    return pd.concat([df.drop(columns=[\\'vectorized\\']), vectorized_df], axis=1)\\n\\n# Pineconeにデータをアップロード\\ndef upload_to_pinecone(index, output_df):\\n    original_texts = output_df.iloc[:, 0]\\n    original_category = output_df.iloc[:, 1]\\n    vectorized_data_only = output_df.iloc[:, 2:]\\n\\n    for i in range(len(vectorized_data_only)):\\n        index.upsert(\\n            vectors=[\\n                {\\n                    \\'id\\': str(i + 1),\\n                    \\'values\\': vectorized_data_only.iloc[i].tolist(),\\n                    \\'metadata\\': {\"text\": original_texts[i], \"Category\": original_category[i]}\\n                }\\n            ]\\n        )\\n\\n# PineconeVectorStoreの初期化\\ndef initialize_vector_store(index_name, embeddings, pinecone_api_key):\\n    return PineconeVectorStore(index_name=index_name, embedding=embeddings, pinecone_api_key=pinecone_api_key)\\n\\n# 質問応答システムのセットアップ\\ndef setup_qa_system(vectorstore):\\n    llm = ChatOpenAI(model_name=\\'gpt-3.5-turbo\\', temperature=0.0)\\n    return RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\\n\\n# メイン処理\\ndef main():\\n    filepath = \\'/Users/ito_itsuki/Documents/python_env/chatbot/data/my_profile.csv\\'\\n    index_name = \\'sample-db\\'\\n    \\n    df = load_data(filepath)\\n    \\n    # ベクトル化と展開\\n    embeddings = OpenAIEmbeddings()\\n    vectorized_df = expand_vectors(vectorize_text(df, embeddings))\\n    \\n    # Pineconeにアップロード\\n    pc = Pinecone(api_key=pinecone_api_key)\\n    pinecone_index = pc.Index(index_name)\\n    upload_to_pinecone(pinecone_index, vectorized_df)\\n    \\n    # 質問応答システムのセットアップと実行\\n    vectorstore = initialize_vector_store(index_name, embeddings, pinecone_api_key)\\n    qa = setup_qa_system(vectorstore)\\n    \\n    query = \\'研究室とかに所属してますか？\\'\\n    result = qa.invoke({\"query\": query})\\n    display(\"Answer:\", result[\\'result\\'])\\n\\nif __name__ == \"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# CSVファイルを読み込み、列名を指定\n",
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath, header=None, names=['text', 'Category'])\n",
    "\n",
    "# テキストデータをベクトル化\n",
    "def vectorize_text(df, embeddings):\n",
    "    df['vectorized'] = df['text'].apply(lambda x: embeddings.embed_query(x))\n",
    "    return df\n",
    "\n",
    "# ベクトルデータをDataFrameに展開\n",
    "def expand_vectors(df):\n",
    "    vectorized_df = pd.DataFrame(df['vectorized'].tolist(), index=df.index)\n",
    "    return pd.concat([df.drop(columns=['vectorized']), vectorized_df], axis=1)\n",
    "\n",
    "# Pineconeにデータをアップロード\n",
    "def upload_to_pinecone(index, output_df):\n",
    "    original_texts = output_df.iloc[:, 0]\n",
    "    original_category = output_df.iloc[:, 1]\n",
    "    vectorized_data_only = output_df.iloc[:, 2:]\n",
    "\n",
    "    for i in range(len(vectorized_data_only)):\n",
    "        index.upsert(\n",
    "            vectors=[\n",
    "                {\n",
    "                    'id': str(i + 1),\n",
    "                    'values': vectorized_data_only.iloc[i].tolist(),\n",
    "                    'metadata': {\"text\": original_texts[i], \"Category\": original_category[i]}\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# PineconeVectorStoreの初期化\n",
    "def initialize_vector_store(index_name, embeddings, pinecone_api_key):\n",
    "    return PineconeVectorStore(index_name=index_name, embedding=embeddings, pinecone_api_key=pinecone_api_key)\n",
    "\n",
    "# 質問応答システムのセットアップ\n",
    "def setup_qa_system(vectorstore):\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.0)\n",
    "    return RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "# メイン処理\n",
    "def main():\n",
    "    filepath = '/Users/ito_itsuki/Documents/python_env/chatbot/data/my_profile.csv'\n",
    "    index_name = 'sample-db'\n",
    "    \n",
    "    df = load_data(filepath)\n",
    "    \n",
    "    # ベクトル化と展開\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorized_df = expand_vectors(vectorize_text(df, embeddings))\n",
    "    \n",
    "    # Pineconeにアップロード\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    pinecone_index = pc.Index(index_name)\n",
    "    upload_to_pinecone(pinecone_index, vectorized_df)\n",
    "    \n",
    "    # 質問応答システムのセットアップと実行\n",
    "    vectorstore = initialize_vector_store(index_name, embeddings, pinecone_api_key)\n",
    "    qa = setup_qa_system(vectorstore)\n",
    "    \n",
    "    query = '研究室とかに所属してますか？'\n",
    "    result = qa.invoke({\"query\": query})\n",
    "    display(\"Answer:\", result['result'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
